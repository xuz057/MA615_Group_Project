---
title: "Textmining exercise"
author: "Xuan,Megha,Yifu,Sky"
date: "November 4, 2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
pacman::p_load(
  "ggplot2",
  "knitr",
  "VGAM",
  "rvest",
  "tidyverse",
  "wordcloud",
  "dplyr",
  "tidytext",
  "stringr",
  "tidyr",
  "scales",
  "gridExtra"
)
```
##SCRAPING DATA FROM https://correlaid.org/blog/

The task of this project is to select two blog entries and conduct text mining based on them.

Our first selected blog entry is about a point pattern analysis on an airplane accident in Florida. The second blog entry is about an AI model developed for game Tic Tac Toe.

```{r readtext,echo=FALSE,warning=FALSE}
#get text from the blog
#read text
blog1 <- read_html("https://correlaid.org/blog/posts/point-pattern-analysis")
blog2 <- read_html("https://correlaid.org/blog/posts/tic-tac-toe-ai")
read_blog1 <- blog1 %>%
 html_nodes("p") %>%
 html_text()
read_blog2 <- blog2 %>%
 html_nodes("p") %>%
 html_text()
```

## GET A TIDY TEXT FORMAT & WORD COUNT
```{r,echo=FALSE,warning=FALSE,results='hide',fig.width=10,fig.height=5}
#make raw text as data frame
text1 <- data_frame(line = 1:37, text = read_blog1)
text2 <- data_frame(line = 1:24, text = read_blog2)
#remove empty lines
text1 <- text1 %>% filter(text != "")
text2 <- text2 %>% filter(text != "")
#a token per row
text1 <-text1 %>%unnest_tokens(word,text)
text2 <-text2 %>%unnest_tokens(word,text)
#get rid of any non-characters
text1 <- text1 %>%mutate(word = str_extract(word,"[a-z']+"))
text1 <-na.omit(text1)
text2 <- text2 %>%mutate(word = str_extract(word,"[a-z']+"))
text2 <-na.omit(text2)
#get rid of stop-words
text1<- text1 %>% anti_join(stop_words)
text2<- text2 %>% anti_join(stop_words)
#word count

p1<-text1 %>%
count(word, sort = TRUE)%>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n))%>%
  ggplot(aes(word, n)) +geom_col() +xlab(NULL) +coord_flip() +ggtitle("Word Count for blog 1")

p2<-text2 %>%
count(word, sort = TRUE)%>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +geom_col() +xlab(NULL) +coord_flip() + ggtitle("Word Count for blog 2")
grid.arrange(p1, p2, ncol=2)
```

After cleaning the texts into a tidy text format, getting rid of non characters and stop words, we did word count analysis on both blogs, which gives us the words from each text with the highest frequency.

From the plot "Word Count for blog 1" we can see that, the most frequently mentioned words are "accidents", "spatial", "pattern", "airplane", "florida", etc. Those are the keywords that describe the main topic of this blog entry.

Similarly, with "Word Count for blog 2", the top words are "ai", "game", "random", etc. That is because this blog is mainly about an AI developed for a game.


```{r,echo=FALSE,warning=FALSE,fig.width=10,fig.height=5}
#Comparing the word frequencies of text1 & text2
frequency <- bind_rows(mutate(text1, author = "Lisa"),
                       mutate(text2, author = "Johannes")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = Johannes, y =Lisa, color = abs(Lisa - Johannes))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") + ggtitle("Comparing Word frequencies")
```

This plot compares the word frequencies from both blogs. The X axis "Johannes" refers to the author of blog 2 while the Y axis "Lisa" refers to the author of blog 1. 

From the plot we could see that both blogs use words "function", "figure" at the same frequency level, largely due to the fact that both of them are science blogs. Blog 1 uses words "data", "values" more often than blog 2 while blog 2 uses words random, board more frequently than blog 1. We think this is because blog 1 is about the analysis based on a case with some data, while blog 2 is about AI program that plays a game with more randomness during the development process.

##Sentiment Analysis With Tidy Data

```{r,echo=FALSE,warning=FALSE,,fig.width=10,fig.height=5}
text_total <- bind_rows(mutate(text1, author = "Lisa"),
                       mutate(text2, author = "Johannes")) 

sentiment <- text_total %>%
  inner_join(get_sentiments("bing")) %>%
  count(author, index = line %/% 2, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(sentiment, aes(index, sentiment, fill = author)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~author, ncol = 2, scales = "free_x") + ggtitle("Sentiment Analysis with Bing")
```


The sentiment analysis based on dictionary bing shows that blog 2 with author Johannes uses more positive words while blog 1 is more balanced with positive and negative words. 

This makes sense since blog 1 is an analysis based on a case study while blog 2 is building an AI model playing a game, which seems a lot more fun.


#Comparing the three sentiment dictionaries

```{r,echo=FALSE,warning=FALSE,fig.width=10,fig.height=6}
#use text1 here as an example
afinn <- text1 %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = line %/% 2) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")
bing_and_nrc <- bind_rows(text1 %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          text1 %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = line %/% 2, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") + ggtitle("Three Dictionaries Sentiment Analysis on Blog 1")
```

To compare Dictionary Bing with two other dictionaries AFINN AND NRC, we used blog 1 as an example. 

From the plot, we could see that the analysis using AFINN dictionary gives most negative results, the analysis using NRC dictionary gives mostly positive results, while the analysis using Bing gives a result balanced from positive and negative. 

##Most common positive and negative words
```{r,echo=FALSE,warning=FALSE,fig.width=10,fig.height=4}
#use text2 here as an example
bing_word_counts <- text2 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(2) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip() + ggtitle("Most Common Positive and Negative Words in Blog 2")
```

The result of the most common positive and negative words in blog 2 is obvious. Since blog 2 is about gaming AI, the common positive words would be "win" and the common negative words would be "lose" or "die" with no doubt.


#Word Cloud
```{r,echo=FALSE,warning=FALSE,,fig.width=3,fig.height=3}
#text1 %>%
#  anti_join(stop_words) %>%
#  count(word) %>%
#  with(wordcloud(word, n, max.words = 100)) 
text2 %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```


The word cloud analysis also gives us a sense of what the blog is mainly about. Take blog 2 as an example. We can clearly see that the blog is about a gaming AI model. 

##Bigram sentiment Analysis

```{r,echo=FALSE,warning=FALSE,,fig.width=3,fig.height=3}
#taking bigram and filtering out non character and OAs
text11<-data_frame(line = 1:24, text = read_blog2)%>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  mutate(word1 = str_extract(word1,"[a-z']+"))%>%
  na.omit(word1)%>%
  mutate(word2 = str_extract(word2,"[a-z']+"))%>%
  na.omit(word2)

#filter out bigrams starts with not
not_words <- text11 %>%
  filter(word1 =="not") %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()
#plot not words
not_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()

```

To take a further look at sentiment analysis, we took blog 2 to look at bigrams. Since the blog isn't very long, we can see from the plot that the bigram begining with "not" is mostly not winning, which is of course a negative bigram.